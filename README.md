# ğŸ›¡ï¸ Risk Analysis Agent

A domain-focused extension of your Finance RAG agent: it adds a **risk classifier layer** on top of retrieval so outputs are **structured, explainable, and auditable** for PMs and Risk teams.

## ğŸ¯ Business goal
Turn dense disclosures (10-K Item 1A, KIIDs, prospectuses) into **actionable, comparable risk insights**:
- Classify chunks into a **risk taxonomy**
- Summarize top risks with **citations**
- Enable **Q&A** with sources
- (Optional) Compare **YoY changes**

**Personas:** Portfolio Managers, Traders, CRO/Risk, Product.

---

## ğŸ§± Architecture

TXT filings (issuer/year/*.txt)
â”‚
â–¼
[Ingest] â”€â”€â–º chunked docs (text + metadata)
â”‚ â”‚
â”‚ â””â”€â”€â–º [Chroma Vector Store] (embeddings + metadata)
â”‚ â–²
â”‚ â”‚
â””â”€â”€â–º [Zero-Shot Risk Classifier] â—„â”€â”˜ (taxonomy tags per chunk)

UI (Streamlit)
â”œâ”€ Ingest: index corpus
â”œâ”€ Analyze: retrieve top chunks â†’ classify â†’ summarize (citations)
â””â”€ Q&A: RAG answers + sources

LLM (Ollama/Mistral) â€” local, free
Embeddings (MiniLM) â€” local, free
Zero-shot (BART-MNLI) â€” local, free


---

## âœ¨ Features
- Ingest `.txt` filings and **chunk** with provenance (issuer, year, file, chunk_id)
- **Local embeddings** â†’ **Chroma** persistent vector DB
- **Zero-shot risk classification** (BART-MNLI) across a **risk taxonomy**
- **Executive summary** grouped by categories with **citations**
- **Q&A** over the corpus with sources
- Fully **local & free** (Ollama + HF models)

---

## ğŸš€ Quick start

```bash
git clone <repo> risk-analysis-agent
cd risk-analysis-agent
python -m venv .venv && . .venv/Scripts/activate   # (Windows)
pip install -r requirements.txt
cp .env.example .env
```

# Add data:
data/samples/ACME_CORP/2024/item_1a.txt  (plain text)

streamlit run app/ui_streamlit.py

# Risk taxonomy

Default labels:

Market, Liquidity, Credit, Operational, Cybersecurity, Regulatory/Legal,
Supply Chain, ESG/Climate, Reputational, Model Risk

Edit in app/taxonomy.py.

## ğŸ” Zero-shot classification (how it works)

We use a pretrained NLI model (BART-MNLI).
For each chunk and label, we score entailment of the hypothesis:

â€œThis text is about <label>.â€
The entailment probability becomes the label score.
Top-k labels are shown per chunk with confidence.

## ğŸ”® Classifier roadmap (free, API-less)

Phase 1 â€“ Zero-Shot Baseline (current)

facebook/bart-large-mnli (free, local)

No training needed, good baseline

Phase 2 â€“ Few-Shot Prompting (upgrade)

Use local LLM (Mistral via Ollama)

Add labeled examples in prompt for domain adaptation

Phase 3 â€“ Fine-Tuned Local Classifier

Train RoBERTa/FinBERT locally on small labeled set (200â€“500 chunks)

Phase 4 â€“ Hybrid / Rules

Add keyword/regex rules for transparency + combine with ML scores

## ğŸ§ª Repo layout
app/
  â”œâ”€ taxonomy.py      # labels & helpers
  â”œâ”€ ingest.py        # read .txt, chunk, normalize
  â”œâ”€ embeddings.py    # MiniLM embeddings (HF)
  â”œâ”€ retriever.py     # Chroma index + retriever
  â”œâ”€ classifier.py    # zero-shot NLI tags
  â”œâ”€ prompts.py       # summary + QA prompts (citations)
  â”œâ”€ llm.py           # Ollama (Mistral)
  â””â”€ ui_streamlit.py  # Streamlit UI

scripts/
  â””â”€ ingest_cli.py

data/
  â””â”€ samples/         # place issuer/year/*.txt here

## ğŸ§° Notes

For PDFs, convert to .txt with an external tool (e.g., pdfminer.six, pypdf) before ingestion.

Set CHROMA_PERSIST_DIR in .env to control the DB location.

Keep LLM_TEMPERATURE low (0.0â€“0.2) for consistent summaries.

## ğŸ“œ License

MIT
